# --- Experiment configurations --------------------------------------------------------------------

# experiment name, used as folder name
experiment_name: mclstm_reference

run_dir: ./runs/mclstm

# files to specify training, validation and test basins (relative to code root or absolute path)
train_basin_file: ./data/447_basin_list.txt
validation_basin_file: ./data/447_basin_list.txt
test_basin_file: ./data/447_basin_list.txt

# training, validation and test time periods (format = 'dd/mm/yyyy')
train_start_date: "01/10/1999"
train_end_date: "30/09/2008"
validation_start_date: "01/10/1980"
validation_end_date: "30/09/1989"
test_start_date: "01/10/1989"
test_end_date: "30/09/1999"

seed:

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu or None]
device: cuda:0

# --- Validation configuration ---------------------------------------------------------------------

# specify after how many epochs to perform validation
validate_every: 3

# specify how many random basins to use for validation
validate_n_random_basins: 447

# specify which metrics to calculate during validation (see neuralhydrology.evaluation.metrics)
# this can either be a list or a dictionary. If a dictionary is used, the inner keys must match the name of the
# target_variable specified below. Using dicts allows for different metrics per target variable.
metrics:
  - NSE

# --- Model configuration --------------------------------------------------------------------------

# base model type [lstm, ealstm, cudalstm, embcudalstm, mtslstm]
# (has to match the if statement in modelzoo/__init__.py)
model: mclstm

head: regression

# ----> General settings <----

# Number of cell states of the LSTM
hidden_size: 64

# Initial bias value of the forget gate
initial_forget_bias: 3

# --- Training configuration -----------------------------------------------------------------------

# specify optimizer [Adam]
optimizer: Adam

# specify loss [MSE, NSE, RMSE]
loss: NSE

# specify learning rates to use starting at specific epochs (0 is the initial learning rate)
learning_rate:
  0: 0.01
  20: 0.005
  25: 0.001

# Mini-batch size
batch_size: 256

# Number of training epochs
epochs: 30

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1

# Defines which time steps are used to calculate the loss. Can't be larger than seq_length.
# If use_frequencies is used, this needs to be a dict mapping each frequency to a predict_last_n-value, else an int.
predict_last_n: 1

# Length of the input sequence
# If use_frequencies is used, this needs to be a dict mapping each frequency to a seq_length, else an int.
seq_length: 365

# Number of parallel workers used in the data pipeline
num_workers: 8

# Log the training loss every n steps
log_interval: 5

# If true, writes logging results into tensorboard file
log_tensorboard: True

# Save model weights every n epochs
save_weights_every: 1

# --- Data configurations --------------------------------------------------------------------------

# which data set to use [camels_us, camels_gb, hourly_camels_us]
dataset: camels_us

# Path to data set root
data_dir: ./data/datadir/CAMELS_US

static_attributes:
  - elev_mean
  - slope_mean
  - area_gages2
  - frac_forest
  - lai_max
  - lai_diff
  - gvf_max
  - gvf_diff
  - soil_depth_pelletier
  - soil_depth_statsgo
  - soil_porosity
  - soil_conductivity
  - max_water_content
  - sand_frac
  - silt_frac
  - clay_frac
  - carbonate_rocks_frac
  - geol_permeability
  - p_mean
  - pet_mean
  - aridity
  - frac_snow
  - high_prec_freq
  - high_prec_dur
  - low_prec_freq
  - low_prec_dur

forcings:
  - maurer_extended

mass_inputs:
  - prcp(mm/day)

dynamic_inputs:
  - srad(W/m2)
  - tmax(C)
  - tmin(C)
  - vp(Pa)

target_variables:
  - QObs(mm/d)

clip_targets_to_zero:
  - QObs(mm/d)

custom_normalization:
  prcp(mm/day):
    centering: None
    scaling: None
  QObs(mm/d):
    centering: None
    scaling: None